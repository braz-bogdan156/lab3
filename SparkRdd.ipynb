from pyspark import SparkConf, SparkContext
import os
number_cores = 2
memory_gb = 4
is_full_dataset = False
conf = (
    SparkConf()
        .setAppName("Spark Rdd Task")
        .setMaster(f'local[{number_cores}]')
        .set('spark.driver.memory', f'{memory_gb}g')
)

sc = SparkContext(conf=conf)


inputRdd = sc.textFile("/Reviews.csv") \
             .map(lambda line: line.split(",")) \
             .map(lambda x: (x[2], x[1]))  


user_products_rdd = inputRdd.groupByKey().mapValues(lambda x: list(set(x)))


product_pairs_rdd = user_products_rdd.flatMapValues(lambda products: [(products[i], products[j]) for i in range(len(products)) for j in range(i + 1, len(products))]) \
                                    .map(lambda x: (tuple(sorted(x[1])), 1)) \
                                    .reduceByKey(lambda x, y: x + y)


top_product_pairs = product_pairs_rdd.takeOrdered(10, key=lambda x: -x[1])


with open("output.txt", "w") as f:
    for pair, count in top_product_pairs:
        f.write(f"{pair}, {count}\n")

sc.stop()
